# Production pretraining configuration
# Matches hierarchical.yaml architecture for consistency

experiment:
  union_type: "standard"  # a+b+d  
  fold_type: "exp1_sequence_fold"
  dataset_size: "full_10fold"

model:
  type: "bert"
  mode: "pretrain"  # Key difference from classification configs
  hidden_size: 512
  num_hidden_layers: 10
  num_attention_heads: 8
  intermediate_size: 2048
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 502
  vocab_size: 69  # 4^3 + 5 special tokens
  mlm_dropout_rate: 0.1

preprocessing:
  tokenizer: "kmer"
  kmer_size: 3
  stride: 3
  max_length: 1500
  padding: "max_length"
  truncation: true
  base_modification_probability: 0.01
  alphabet: ["A","C","G","T"]
  masking_percentage: 0.15  # Standard BERT masking

training:
  batch_size: 162  # From old production config
  learning_rate: 0.00018  # Peak LR from old config  
  weight_decay: 0.01
  warmup_steps: 350  # From old config
  max_epochs: 100  # Reasonable for production (was 600 in old config)
  
  patience: 20  # From old config (was 100)
  min_delta: 0.001
  
  accumulation_steps: 1
  device: "cuda"
  num_workers: 4
  mixed_precision: true
  
  log_frequency: 100
  verbose: true

seed: 42