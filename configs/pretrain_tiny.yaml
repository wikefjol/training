# Tiny pretraining configuration for testing
# Matches tiny_hierarchical.yaml architecture for consistency

experiment:
  union_type: "standard"  # a+b+d
  fold_type: "exp1_sequence_fold"
  dataset_size: "debug_5genera_10fold"

model:
  type: "bert"
  mode: "pretrain"  # Key difference from classification configs
  hidden_size: 64
  num_hidden_layers: 2
  num_attention_heads: 2
  intermediate_size: 256
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 128  # Match tiny_hierarchical
  vocab_size: 69  # 4^3 + 5 special tokens
  mlm_dropout_rate: 0.1

preprocessing:
  tokenizer: "kmer"
  kmer_size: 3
  stride: 3
  max_length: 128  # Match tiny_hierarchical
  padding: "max_length"
  truncation: true
  base_modification_probability: 0.01
  alphabet: ["A","C","G","T"]
  masking_percentage: 0.15  # Standard BERT masking

training:
  batch_size: 16
  learning_rate: 0.00005
  weight_decay: 0.01
  warmup_steps: 100
  max_epochs: 20  # Reasonable for tiny pretraining
  
  patience: 5
  min_delta: 0.001
  
  accumulation_steps: 1
  device: "cuda"
  num_workers: 0
  mixed_precision: false
  
  log_frequency: 10
  verbose: true

seed: 42