# Tiny model configuration for testing on login nodes
# This uses minimal resources and runs quickly

experiment:
  union_type: "standard"  # a+b+d
  fold_type: "exp1_sequence_fold"
  dataset_size: "debug_5genera_10fold"  # ~10k sequences
  num_folds: 10
  folds_to_train: [1]  # Just test one fold

model:
  type: "bert"
  vocab_size: 256  # Very small vocab
  hidden_size: 64  # Tiny hidden size
  num_hidden_layers: 2  # Just 2 layers
  num_attention_heads: 2  # Minimal heads
  intermediate_size: 256  # 4x hidden
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 128  # Shorter sequences

preprocessing:
  tokenizer: "kmer"
  kmer_size: 3  # Smaller k-mers for tiny vocab
  stride: 1
  max_length: 128  # Shorter for faster processing
  padding: "max_length"
  truncation: true

training:
  batch_size: 16  # Small batch
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 10
  max_epochs: 2  # Just 2 epochs for testing
  
  patience: 5  # No early stopping for test
  min_delta: 0.001
  monitor: "val_accuracy"
  
  save_best_only: true
  save_frequency: 1
  
  device: "cpu"  # Use CPU on login node
  num_workers: 0  # No multiprocessing on login node
  mixed_precision: false
  
  log_frequency: 10
  verbose: true

seed: 42